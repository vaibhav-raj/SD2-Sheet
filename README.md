| Sl.No|  NodeJS Questions       |
|------|------------------|
| 01. | [What is Node.js and why is it used in web development?](#q-what-is-nodejs-and-why-is-it-used-in-web-development)  
| 02. | [Explain the concept of non-blocking I/O in Node.js.](#q-explain-the-concept-of-non-blocking-io-in-nodejs)  
| 03. | [What is npm and how do you use it to manage dependencies in a Node.js project?](#q-what-is-npm-and-how-do-you-use-it-to-manage-dependencies-in-a-nodejs-project)  
| 04. | [What is the event loop in Node.js and how does it work?](#q-what-is-the-event-loop-in-nodejs-and-how-does-it-work)  
| 05. | [How do you handle asynchronous operations in Node.js?](#q-how-do-you-handle-asynchronous-operations-in-nodejs)  
| 06. | [What is Express.js and how does it relate to Node.js?](#q-what-is-expressjs-and-how-does-it-relate-to-nodejs)  
| 07. | [What is a callback function in Node.js? Can you provide an example of its usage?](#q-what-is-a-callback-function-in-nodejs-can-you-provide-an-example-of-its-usage)  
| 08. | [Explain the difference between setTimeout() and setInterval() functions in Node.js.](#q-explain-the-difference-between-settimeout-and-setinterval-functions-in-nodejs)  
| 09. | [What is middleware in the context of Express.js?](#q-what-is-middleware-in-the-context-of-expressjs)  
| 10. | [How do you handle errors in a Node.js application?](#q-how-do-you-handle-errors-in-a-nodejs-application)  
| 11. | [Explain the purpose of package.json in a Node.js project.](#q-explain-the-purpose-of-packagejson-in-a-nodejs-project)  
| 12. | [What are modules in Node.js? How do you create and use them?](#q-what-are-modules-in-nodejs-how-do-you-create-and-use-them)  
| 13. | [How do you read and write files asynchronously in Node.js?](#q-how-do-you-read-and-write-files-asynchronously-in-nodejs)  
| 14. | [What are the differences between let, const, and var in JavaScript?](#q-what-are-the-differences-between-let-const-and-var-in-javascript)  
| 15. | [How do you debug a Node.js application?](#q-how-do-you-debug-a-nodejs-application)  
| 16. | [Explain the concept of streams in Node.js.](#q-explain-the-concept-of-streams-in-nodejs)  
| 17. | [What is RESTful API and how would you implement it in Node.js using Express?](#q-what-is-restful-api-and-how-would-you-implement-it-in-nodejs-using-express)  
| 18. | [How do you handle form data submission in Node.js?](#q-how-do-you-handle-form-data-submission-in-nodejs)  
| 19. | [What is CORS and how do you handle it in a Node.js application?](#q-what-is-cors-and-how-do-you-handle-it-in-a-nodejs-application)  
| 20. | [Explain the difference between require() and import in Node.js.](#q-explain-the-difference-between-require-and-import-in-nodejs)  
| 21. | [What is event-driven programming in Node.js? How does it differ from traditional programming?](#q-what-is-event-driven-programming-in-nodejs-how-does-it-differ-from-traditional-programming)  
| 22. | [What are the differences between process.nextTick() and setImmediate()? When would you use each?](#q-what-are-the-differences-between-processnexttick-and-setimmediate-when-would-you-use-each)  
| 23. | [What is callback hell and how do you avoid it in Node.js?](#q-what-is-callback-hell-and-how-do-you-avoid-it-in-nodejs)  
| 24. | [What is the role of EventEmitter in Node.js? Can you provide an example of how to use it?](#q-what-is-the-role-of-eventemitter-in-nodejs-can-you-provide-an-example-of-how-to-use-it)  
| 25. | [What are the differences between fork, spawn, and exec in Node.js? When would you use each?](#q-what-are-the-differences-between-fork-spawn-and-exec-in-nodejs-when-would-you-use-each)  
| 26. | [What are the security best practices to follow when developing a Node.js application?](#q-what-are-the-security-best-practices-to-follow-when-developing-a-nodejs-application)  
| 27. | [What is clustering in Node.js and how does it improve performance?](#q-what-is-clustering-in-nodejs-and-how-does-it-improve-performance)  
| 28. | [What is JWT authentication and how would you implement it in a Node.js application?](#q-what-is-jwt-authentication-and-how-would-you-implement-it-in-a-nodejs-application)  
| 29. | [How do you handle file uploads in Node.js?](#q-how-do-you-handle-file-uploads-in-nodejs)  
| 30. | [Difference between package.json and package-lock.json](#q-difference-between-packagejson-and-package-lockjson)  
| 31. | [When should you use npm and when yarn?](#q-when-should-you-use-npm-and-when-yarn)  
| 32. | [What is REPL? What purpose is it used for?](#q-what-is-repl-what-purpose-it-is-used-for)  
| 33. | [What is the purpose of the global object in Node.js?](#q-what-is-the-purpose-of-the-global-object-in-nodejs)  
| 34. | [Explain the concept of child processes in Node.js. How would you create and manage them?](#q-explain-the-concept-of-child-processes-in-nodejs-how-would-you-create-and-manage-them)  
| 35. | [What are the different HTTP methods in Node.js? How would you handle each of them in an Express.js application?](#q-what-are-the-different-http-methods-in-nodejs-how-would-you-handle-each-of-them-in-an-expressjs-application)  
| 36. | [What is the role of module.exports and exports in Node.js modules? How do you use them?](#q-what-is-the-role-of-moduleexports-and-exports-in-nodejs-modules-how-do-you-use-them)  
| 37. | [What are the benefits of using TypeScript with Node.js? How would you set up a Node.js project with TypeScript?](#q-what-are-the-benefits-of-using-typescript-with-nodejs-how-would-you-set-up-a-nodejs-project-with-typescript)  
| 38. | [What is the purpose of the __dirname and __filename variables in Node.js?](#q-what-is-the-purpose-of-the-dirname-and-filename-variables-in-nodejs)  
| 39. | [How would you handle environment variables in a Node.js application?](#q-how-would-you-handle-environment-variables-in-a-nodejs-application)   
| 40. | [What are buffers and why do we need to use them with Node.js?](#q-what-are-buffers-and-why-do-we-need-to-use-them-with-node-js)  
| 41. | [What are `express.json()` and `express.urlencoded()` in Express.js?](#q-what-are-expressjson-and-expressurlencoded-in-expressjs)  
| 42. | [Explain what is wrong with async/await use in a forEach loop?](#q-explain-what-is-wrong-with-asyncawait-use-in-a-foreach-loop)  
| 43. | [How would you read files in parallel in Node.js? Provide a code example.](#q-how-would-you-read-files-in-parallel-in-node-js-provide-code-example)  
| 44. | [How would you read files sequentially in Node.js? Provide a code example.](#q-how-would-you-read-files-sequentially-in-node-js-provide-code-example)  
| 45. | [When is it best to use `module.exports` vs a class or an object literal when defining a Node.js module?](#q-when-is-it-best-to-use-moduleexports-vs-a-class-or-an-object-literal-when-defining-a-nodejs-module)  
| 46. | [How do I convert an existing callback API to promises?](#q-how-do-i-convert-an-existing-callback-api-to-promises)  

## Table of Contents

* *[MongoDB Interview Questions](mongodb-questions.md)*
* *[NestJS Interview Questions](nestjs.md)*
* *[GraphQl Interview Questions](graphql.md)*
* *[Redis Cache Interview Questions](redis-cache.md)*
* *[DSA Top Interview 150](DSA-150.md)*
  

## Q. ***What does the runtime environment mean in Node.js?***
In the context of Node.js, the "runtime environment" refers to the environment where Node.js code is executed. 
It includes everything needed to run a Node.js application, such as the V8 JavaScript engine, the Node.js runtime, and various libraries and modules.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is Node.js and why is it used in web development?***

Node.js is an open-source server side runtime environment built on Chrome\'s V8 JavaScript engine. 
It provides an event driven, non-blocking (asynchronous) I/O and cross-platform runtime environment for building highly scalable server-side applications using JavaScript.

### Key Features

- **Asynchronous & Non-Blocking**: Efficiently handles concurrent connections without waiting.
- **Fast Execution with V8 Engine**: Built on Google's V8 JavaScript runtime engine for fast code execution.
- **Unified Language (JavaScript)**: Promotes code reuse and consistency by using JavaScript on both server and client sides.
- **NPM**: Robust package manager with a vast ecosystem of open-source libraries and modules.
- **Community Support**: Large and active developer community provides resources, libraries, and tools.
  
### Use Cases

- **Data Streaming**: Suited for real-time streaming of audio, video, and lightweight data.
- **API Servers**: Ideal for building fast, scalable, and data-intensive applications.
- **Microservices**: Its modular design makes it easy to create separate, scalable services that can operate independently.
- **Single Page Applications**: Often used with frameworks like Angular, React, or Vue to craft robust, server-side backends.
- **Chat Applications**: Its real-time capabilities are advantageous in building instant messaging systems.
- **Internet of Things (IoT)**: Offers a lightweight setup for running apps on limited devices like Raspberry Pi.

### Why is it used in web development?

- **Efficient Handling of Concurrent Requests**: Node.js efficiently manages multiple connections simultaneously, ideal for responsive and scalable applications.
- **Unified Language for Server and Client**: With JavaScript for both server and client-side, developers enjoy consistency and code reuse across the entire application.
- **Fast Execution**: Powered by the swift V8 JavaScript engine, Node.js swiftly executes tasks, perfect for performance-focused operations.
- **Rich Ecosystem with NPM**: Node Package Manager (NPM) boasts a vast library of reusable modules, simplifying integration of third-party functionalities.
- **Real-time Application Capabilities**: Node.js shines in real-time applications, crucial for instant data updates in scenarios like chat apps, online gaming, and collaboration tools.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is Node.js Process Model?***

The Node.js process model revolves around a single-threaded, event-driven architecture. Here's a breakdown:

1. **Single-threaded Event Loop**: Node.js operates on a single-threaded event loop. This means that it handles all operations, including I/O tasks, asynchronously through event callbacks. It doesn't create new threads for every request, which makes it highly efficient for handling numerous concurrent connections.

2. **Non-blocking I/O Operations**: Node.js utilizes non-blocking I/O operations, meaning that while an I/O task (like reading from a file or making a network request) is being processed, the event loop continues to run, allowing other operations to proceed without being blocked. This enables Node.js to handle many concurrent connections without incurring the overhead of thread creation and management.

3. **Event-driven Programming**: Node.js follows an event-driven programming paradigm. It uses event emitters to trigger events, and listeners to respond to those events. This pattern allows developers to write highly scalable and responsive applications by handling events asynchronously.

4. **Worker Pool for CPU-bound Tasks**: While Node.js is single-threaded and optimized for I/O-bound tasks, it may not be suitable for CPU-bound operations that require heavy computation. For such tasks, Node.js provides a worker pool, allowing developers to offload CPU-intensive work to separate threads or processes, while still benefiting from the event-driven architecture for handling I/O tasks.

------------------------------------------------

Node.js operates on a single-threaded, event-driven model:
- It uses a single thread to handle multiple tasks, making it efficient.
- Non-blocking I/O allows it to handle many connections simultaneously without waiting.
- Events trigger actions, making Node.js responsive and scalable.
- For heavy computations, it provides a worker pool to offload tasks.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the concept of non-blocking I/O in Node.js.***

Non-blocking I/O in Node.js means that when an I/O operation, like reading from a file or making a network request, is initiated,
Node.js doesn't wait for it to finish before moving on to the next task. 
Instead, it continues executing other operations while the I/O operation is being processed asynchronously in the background. 

This approach allows Node.js to handle multiple I/O tasks concurrently without getting blocked, maximizing efficiency and responsiveness. 
When the I/O operation completes, Node.js triggers a callback to handle the result, ensuring that the application can continue running smoothly without waiting for slow I/O operations to finish. 

In essence, non-blocking I/O enables Node.js to efficiently manage multiple operations simultaneously, making it ideal for handling high-concurrency scenarios, such as web servers serving many clients at once.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is npm and how do you use it to manage dependencies in a Node.js project?***
npm, short for Node Package Manager, is a tool used for managing packages and dependencies in Node.js projects. Here's how it works:

1. **Installing npm**: npm comes bundled with Node.js, so when you install Node.js, npm is automatically installed alongside it.

2. **Initializing a Project**: To start managing dependencies with npm, you first initialize a new Node.js project by creating a `package.json` file. You can do this by running `npm init` in your project directory, which will guide you through creating the `package.json` file.

3. **Installing Dependencies**: Once you have a `package.json` file, you can start adding dependencies to your project. You can install a package and save it as a dependency by running `npm install <package-name> --save`. This will add the package to your `package.json` file under the `dependencies` section and install it in the `node_modules` directory.

4. **Installing Development Dependencies**: You can also install packages that are only needed during development, such as testing frameworks or build tools, by running `npm install <package-name> --save-dev`. These packages will be added to the `devDependencies` section of your `package.json` file.

5. **Using Installed Packages**: Once a package is installed, you can require it in your code just like any other module. Node.js will automatically look in the `node_modules` directory for installed packages when you require them.

6. **Updating Dependencies**: You can update all your project dependencies to their latest versions by running `npm update`. This will update the `package.json` file with the latest versions of your dependencies and install them.

7. **Removing Dependencies**: If you no longer need a dependency, you can remove it from your project by running `npm uninstall <package-name>`.

By using npm to manage dependencies, you can easily add, update, and remove packages from your Node.js project, ensuring that your project stays organized and up-to-date with the latest libraries and tools.

npm (Node Package Manager) is the default package manager for Node.js. It's a command-line tool that allows developers to discover, install, and manage dependencies (third-party libraries, frameworks, and tools) for Node.js projects. npm also provides a registry where developers can publish and share their own packages.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>


## Q. ***What is the event loop in Node.js and how does it work?***

In Node.js, the event loop is a crucial mechanism for handling asynchronous operations efficiently. Here's a simplified explanation of how it works:

1. **Event Loop Basics**: The event loop is a single-threaded mechanism that continuously checks for tasks to execute in a loop. It keeps Node.js running, processing tasks one by one.

2. **Non-Blocking I/O Operations**: When Node.js encounters an asynchronous task, such as reading from a file or making a network request, it delegates the task to the system kernel and moves on to the next task without waiting for the asynchronous operation to complete.

3. **Event Queue**: Asynchronous tasks, along with their associated callback functions, are placed in a queue called the "event queue" when they are initiated.

4. **Execution Order**: Once the synchronous tasks in the call stack are executed, Node.js checks the event queue for pending tasks. If there are any, it retrieves them one by one and executes their associated callback functions.

5. **Callbacks**: Callback functions associated with completed asynchronous tasks are executed in the event loop, allowing Node.js to handle I/O operations efficiently without blocking the execution of other tasks.

6. **Concurrency**: Since Node.js is single-threaded, it can only execute one task at a time. However, by utilizing non-blocking I/O and the event loop, Node.js can handle numerous concurrent operations efficiently.

In summary, the event loop in Node.js enables asynchronous operations to be executed efficiently by continuously checking for pending tasks and executing their associated callback functions in a non-blocking manner. This allows Node.js to handle high-concurrency scenarios and remain responsive while performing I/O-bound operations.

## Q. ***How do you handle asynchronous operations in Node.js?***

In Node.js, asynchronous operations are handled using callbacks, Promises, or async/await syntax. Here's how each approach works:

1. **Callbacks**: Callbacks are functions that are passed as arguments to asynchronous functions. They are invoked when the asynchronous operation completes or encounters an error. Here's an example of handling an asynchronous operation using callbacks:

```javascript
// Asynchronous operation example (reading a file)
const fs = require('fs');

fs.readFile('example.txt', 'utf8', (err, data) => {
  if (err) {
    console.error('Error:', err);
    return;
  }
  console.log('File content:', data);
});
```

2. **Promises**: Promises are objects that represent the eventual completion (or failure) of an asynchronous operation. They provide a cleaner way to handle asynchronous code compared to callbacks, especially when dealing with multiple asynchronous operations sequentially or concurrently. Here's how you can use Promises:

```javascript
// Asynchronous operation example (reading a file) using Promises
const fs = require('fs').promises;

fs.readFile('example.txt', 'utf8')
  .then(data => {
    console.log('File content:', data);
  })
  .catch(err => {
    console.error('Error:', err);
  });
```

3. **Async/await**: Async/await is a modern JavaScript feature that provides syntactic sugar on top of Promises, making asynchronous code look more like synchronous code. Async functions return Promises implicitly, and the await keyword is used to wait for the resolution of a Promise. Here's how you can use async/await:

```javascript
// Asynchronous operation example (reading a file) using async/await
const fs = require('fs').promises;

async function readFileAsync() {
  try {
    const data = await fs.readFile('example.txt', 'utf8');
    console.log('File content:', data);
  } catch (err) {
    console.error('Error:', err);
  }
}

readFileAsync();
```

All three methods—callbacks, Promises, and async/await—serve the purpose of handling asynchronous operations in Node.js. The choice between them often depends on personal preference, project requirements, or compatibility with existing codebases. Promises and async/await are generally preferred due to their cleaner syntax and better error handling compared to traditional callback-based approaches.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is Express.js and how does it relate to Node.js?***

Express.js is a web application framework for Node.js, designed to simplify the process of building web applications and APIs. It provides a set of robust features for web and mobile applications, including routing, middleware, template engines, and more.

Here's how Express.js relates to Node.js:

1. **Built on Node.js**: Express.js is built on top of Node.js, leveraging its asynchronous, event-driven architecture to handle web requests efficiently.

2. **Middleware**: Express.js simplifies the process of handling HTTP requests by providing middleware, which are functions that have access to the request and response objects. Middleware can perform tasks such as parsing request bodies, logging requests, authentication, and error handling.

3. **Routing**: Express.js offers a simple and intuitive routing mechanism that allows developers to define routes for different HTTP methods and URLs. This makes it easy to create APIs and handle different types of requests.

4. **Template Engines**: Express.js supports various template engines, such as Pug, EJS, and Handlebars, allowing developers to generate dynamic HTML content to be served to clients.

5. **Modularity**: Express.js is designed to be modular, allowing developers to add or remove features as needed using middleware and third-party modules. This makes it flexible and customizable for a wide range of web application requirements.

Overall, Express.js complements Node.js by providing a higher-level framework for building web applications and APIs, while still leveraging the underlying capabilities and performance of Node.js. It simplifies common web development tasks and enables developers to create robust and scalable applications more efficiently.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is a callback function in Node.js? Can you provide an example of its usage?***

In Node.js, a callback function is a function that is passed as an argument to another function and is invoked (called back) at a later time, typically after an asynchronous operation has completed. Callback functions are commonly used to handle asynchronous operations such as reading from a file, making a network request, or querying a database.

Here's a simple example demonstrating the usage of a callback function in Node.js:

```javascript
// Define a function that performs an asynchronous operation
function doAsyncTask(callback) {
  setTimeout(() => {
    const data = 'Result of the asynchronous operation';
    callback(data);
  }, 1000); // Simulate a delay of 1 second
}

// Usage of the doAsyncTask function with a callback
console.log('Start of the program');
doAsyncTask((result) => {
  console.log('Received result:', result);
});
console.log('End of the program');
```

In this example:

1. We define a function `doAsyncTask` that takes a callback function `callback` as an argument.
2. Inside `doAsyncTask`, we simulate an asynchronous operation using `setTimeout` to mimic a delay. After the delay, we call the callback function with some data.
3. We call `doAsyncTask` passing an anonymous function as the callback. This function will be executed once the asynchronous operation inside `doAsyncTask` is completed.
4. Before and after calling `doAsyncTask`, we log messages to the console to demonstrate that the program doesn't wait for the asynchronous operation to complete and continues executing other code.

When you run this code, you'll see the following output:

```
Start of the program
End of the program
Received result: Result of the asynchronous operation
```

This demonstrates that the callback function passed to `doAsyncTask` is executed asynchronously, after the rest of the code has been executed. Callbacks are a fundamental aspect of asynchronous programming in Node.js and are widely used throughout the Node.js ecosystem.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the difference between setTimeout() and setInterval() functions in Node.js.***

In Node.js, both `setTimeout()` and `setInterval()` are functions used to execute code asynchronously, but they have different purposes and behaviors:

1. **setTimeout()**:
   - `setTimeout()` is used to execute a function once after a specified delay (in milliseconds).
   - It takes two arguments: a callback function to execute, and the delay (in milliseconds) after which the function should be executed.
   - After the specified delay, the callback function is added to the event queue and will be executed as soon as the call stack is empty.
   - If there are other tasks in the event queue or the call stack when the delay expires, the callback function will wait until those tasks are completed before being executed.
   - Example:
     ```javascript
     setTimeout(() => {
       console.log('This message will be printed after 2000 milliseconds');
     }, 2000);
     ```

2. **setInterval()**:
   - `setInterval()` is used to execute a function repeatedly at specified intervals (in milliseconds).
   - It takes two arguments: a callback function to execute, and the interval (in milliseconds) between each execution of the function.
   - The callback function is executed repeatedly, with each execution separated by the specified interval.
   - Unlike `setTimeout()`, which executes the function only once, `setInterval()` continues to execute the function indefinitely until it's explicitly stopped or the Node.js process exits.
   - Example:
     ```javascript
     const intervalId = setInterval(() => {
       console.log('This message will be printed every 1000 milliseconds');
     }, 1000);
     ```

The main difference between `setTimeout()` and `setInterval()` is that `setTimeout()` executes the function once after a specified delay, while `setInterval()` executes the function repeatedly at specified intervals until explicitly stopped. It's essential to be mindful of the potential performance implications and side effects when using `setInterval()` for long-running tasks or tasks that may overlap. Additionally, it's crucial to remember to clear intervals using `clearInterval()` when they are no longer needed to avoid memory leaks or unexpected behavior.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is middleware in the context of Express.js?***

In the context of Express.js, middleware refers to functions that have access to the request object (`req`), the response object (`res`), and the next middleware function in the application's request-response cycle. Middleware functions can perform tasks such as modifying request and response objects, executing additional code, and terminating the request-response cycle.

Middleware functions in Express.js can be used to:

1. Execute code: Middleware functions can perform various tasks, such as logging requests, parsing request bodies, or performing authentication and authorization checks.

2. Modify request and response objects: Middleware functions can modify request and response objects by adding or modifying properties, headers, or bodies.

3. Execute additional middleware: Middleware functions can invoke the `next()` function to pass control to the next middleware function in the application's middleware stack.

Middleware functions in Express.js can be added globally to the application or locally to specific routes. They are executed in the order in which they are defined in the application's middleware stack.

Here's an example of a simple middleware function in Express.js:

```javascript
const express = require('express');
const app = express();

// Middleware function
const myMiddleware = (req, res, next) => {
  console.log('Middleware function is executed');
  // Pass control to the next middleware function
  next();
};

// Registering middleware globally
app.use(myMiddleware);

// Route handler
app.get('/', (req, res) => {
  res.send('Hello, World!');
});

// Starting the server
app.listen(3000, () => {
  console.log('Server is running on port 3000');
});
```

In this example:

- We define a middleware function `myMiddleware` that logs a message when executed and then calls the `next()` function to pass control to the next middleware function.
- We register the middleware globally using `app.use()`, so it will be executed for all incoming requests.
- When a request is made to the root path (`/`), the middleware function is executed first, followed by the route handler that sends a response of 'Hello, World!'.

Middleware functions provide a powerful mechanism for adding modular and reusable functionality to Express.js applications, allowing developers to enhance the functionality of their applications in a clean and organized manner.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do you handle errors in a Node.js application?***

Handling errors effectively is crucial for building robust Node.js applications. There are several approaches to handle errors in a Node.js application:

1. **Try-Catch**: You can use try-catch blocks to catch synchronous errors within your code. This approach is suitable for handling errors that occur within a specific function or block of code.

   ```javascript
   try {
     // Code that may throw an error
   } catch (error) {
     // Handle the error
   }
   ```

2. **Error-First Callbacks**: For asynchronous operations, such as file I/O or database queries, you can use error-first callbacks convention. This convention involves passing an error object as the first argument to the callback function if an error occurs during the operation.

   ```javascript
   fs.readFile('example.txt', 'utf8', (err, data) => {
     if (err) {
       // Handle the error
     } else {
       // Process the data
     }
   });
   ```

3. **Promises**: When working with Promises, you can use the .catch() method to handle errors that occur during Promise resolution. This approach allows you to centralize error handling for asynchronous operations.

   ```javascript
   someAsyncOperation()
     .then(result => {
       // Process the result
     })
     .catch(error => {
       // Handle the error
     });
   ```

4. **Async/Await**: Async functions in combination with the try-catch syntax provide a more synchronous-like error handling approach for asynchronous code.

   ```javascript
   async function fetchData() {
     try {
       const data = await someAsyncOperation();
       // Process the data
     } catch (error) {
       // Handle the error
     }
   }
   ```

5. **Express.js Error Handling Middleware**: In Express.js applications, you can define error-handling middleware functions with four parameters (err, req, res, next). These middleware functions are used to handle errors that occur during the request-response cycle.

   ```javascript
   app.use((err, req, res, next) => {
     // Handle the error
     res.status(500).send('Internal Server Error');
   });
   ```

Regardless of the approach you choose, it's essential to handle errors gracefully, log relevant information for debugging purposes, and provide appropriate responses to users. Additionally, consider using tools like Winston or Bunyan for logging errors and monitoring your application's health.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the purpose of package.json in a Node.js project.***

Certainly! Here are the purposes of `package.json` in a Node.js project broken down into points:

1. **Dependency Management**:
   - Lists all dependencies required for the project.
   - Separates dependencies into development (`devDependencies`) and production (`dependencies`) categories.

2. **Version Management**:
   - Specifies version ranges or exact versions for each dependency.
   - Ensures consistent dependency versions across different environments.

3. **Script Definitions**:
   - Defines custom scripts for various project tasks.
   - Scripts can include tasks like testing, building, linting, or starting the application.

4. **Metadata**:
   - Contains metadata about the project such as name, version, description, author, and license.
   - Helps users understand the project and its dependencies.

5. **Project Configuration**:
   - Includes project-specific configurations like eslint rules, babel presets, or TypeScript settings.
   - Allows for easy sharing and replication of project settings among team members.

`package.json` serves as a central configuration file for Node.js projects, providing essential information about dependencies, scripts, metadata, and configurations, ensuring consistency and facilitating collaboration among developers.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are modules in Node.js? How do you create and use them?***

In Node.js, modules are reusable blocks of code that encapsulate functionality and can be easily imported and used in other parts of a Node.js application.<br> Modules help organize code, promote code reusability, and improve maintainability.<br> 
**Types of Modules**:
    - **Core Modules**: Built-in modules provided by Node.js, such as `fs`, `http`, and `path`. <br>
    - **User-created Modules**: Custom modules created by developers to encapsulate specific functionality tailored to their application's needs.

Here's how you can create and use modules in Node.js:

1. **Creating a Module**:
   
   To create a module in Node.js, you typically define functionality within a JavaScript file and export it using the `module.exports` or `exports` object.

   Example of a module (`myModule.js`):
   ```javascript
   // Define functionality
   function greet(name) {
       return `Hello, ${name}!`;
   }

   // Export functionality
   module.exports = {
       greet: greet
   };
   ```

2. **Using a Module**:

   Once you've created a module, you can import and use it in other parts of your Node.js application using the `require()` function.

   Example of using the module in another file (`app.js`):
   ```javascript
   // Import the module
   const myModule = require('./myModule');

   // Use functionality from the module
   console.log(myModule.greet('John'));
   ```

   In this example:
   - We import the `myModule` module using `require('./myModule')`. The path to the module file (`'./myModule'`) is relative to the current file.
   - We then use the functionality exported by the module (`myModule.greet('John')`) to greet a user.

3. **Exporting Multiple Functions or Objects**:

   You can export multiple functions or objects from a module by adding them to the `module.exports` or `exports` object.

   Example:
   ```javascript
   // Define multiple functions
   function greet(name) {
       return `Hello, ${name}!`;
   }

   function farewell(name) {
       return `Goodbye, ${name}!`;
   }

   // Export multiple functions
   module.exports = {
       greet: greet,
       farewell: farewell
   };
   ```

   You can then use these functions in other parts of your application in the same way as shown above.

4. **Core Modules**:

   In addition to user-created modules, Node.js provides core modules (built-in modules) that can be imported and used without the need for installation. Core modules are accessed using their module names without specifying a file path.

   Example:
   ```javascript
   // Importing a core module (fs - File System module)
   const fs = require('fs');

   // Using functionality from the core module
   fs.readFile('example.txt', 'utf8', (err, data) => {
       if (err) {
           console.error('Error reading file:', err);
           return;
       }
       console.log('File content:', data);
   });
   ```

   In this example, we import the `fs` core module and use its `readFile` function to read the contents of a file asynchronously.

Node.js modules play a crucial role in structuring and organizing code in Node.js applications. They enable code reuse, promote modularity, and facilitate collaboration among developers.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do you read and write files asynchronously in Node.js?***

In Node.js, you can read and write files asynchronously using the `fs` (File System) module. The `fs` module provides methods for interacting with the file system, including reading and writing files. Here's how you can read and write files asynchronously in Node.js:

1. **Reading Files Asynchronously**:

```javascript
const fs = require('fs');

// Read a file asynchronously
fs.readFile('example.txt', 'utf8', (err, data) => {
    if (err) {
        console.error('Error reading file:', err);
        return;
    }
    console.log('File content:', data);
});
```

In this example:
- We use the `readFile()` method of the `fs` module to read the contents of the file named `example.txt`.
- The second argument `'utf8'` specifies the encoding of the file. If the encoding is not specified, the raw buffer data will be returned.
- The callback function is invoked once the file reading operation is completed. It takes two parameters: `err` (an error object if an error occurred during the operation) and `data` (the contents of the file if the operation was successful).

2. **Writing Files Asynchronously**:

```javascript
const fs = require('fs');

// Write to a file asynchronously
const content = 'Hello, World!';
fs.writeFile('output.txt', content, 'utf8', (err) => {
    if (err) {
        console.error('Error writing file:', err);
        return;
    }
    console.log('File has been written successfully!');
});
```

In this example:
- We use the `writeFile()` method of the `fs` module to write the content `'Hello, World!'` to a file named `output.txt`.
- The second argument `content` contains the data to be written to the file.
- The third argument `'utf8'` specifies the encoding of the data.
- The callback function is invoked once the file writing operation is completed. It takes one parameter: `err` (an error object if an error occurred during the operation).

Both `readFile()` and `writeFile()` methods perform file operations asynchronously, meaning that they do not block the execution of the rest of the code. Instead, they execute in the background, and the callback functions are invoked when the operations are completed, allowing the program to continue executing other tasks in the meantime.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the differences between let, const, and var in JavaScript?***

`let`, `const`, and `var` are all used for variable declaration in JavaScript, but they have some differences in terms of scope, hoisting, and reassignment.

1. **var**:
   - `var` is function-scoped, meaning it is only visible within the function in which it is declared or globally if declared outside of any function.
   - Variables declared with `var` are hoisted to the top of their scope. This means you can access the variable before it's declared, but it will have an initial value of `undefined`.
   - `var` allows for redeclaration and reassignment.
   - Example:
     ```javascript
     function example() {
       if (true) {
         var x = 10;
       }
       console.log(x); // 10
     }
     console.log(x); // ReferenceError: x is not defined
     ```

2. **let**:
   - `let` is block-scoped, meaning it is only visible within the block (e.g., within curly braces) in which it is declared.
   - Variables declared with `let` are not hoisted to the top of their scope. They are only accessible after the declaration.
   - `let` allows for reassignment but not redeclaration within the same scope.
   - Example:
     ```javascript
     function example() {
       if (true) {
         let x = 10;
       }
       console.log(x); // ReferenceError: x is not defined
     }
     ```

3. **const**:
   - `const` is also block-scoped.
   - Variables declared with `const` are not hoisted.
   - `const` variables must be initialized at the time of declaration, and their values cannot be changed (they are immutable).
   - Example:
     ```javascript
     const PI = 3.14;
     PI = 3; // TypeError: Assignment to constant variable.
     ```

In general, it's recommended to use `const` by default for variable declarations unless you know the variable's value will need to change later in the code. Use `let` when you need a variable to be reassigned, and avoid using `var` due to its function-scoped behavior and potential hoisting issues.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do you debug a Node.js application?***

Debugging a Node.js application can be done using various tools and techniques. Here are some common methods for debugging Node.js applications:

1. **console.log() Statements**: One of the simplest ways to debug a Node.js application is by using `console.log()` statements to output values to the console. This allows you to inspect the flow of execution and the values of variables at different points in your code.

2. **Node.js Inspector**: Node.js comes with a built-in debugger called Node.js Inspector. You can enable it by running your script with the `--inspect` or `--inspect-brk` flag followed by the filename. This will start the Inspector server, and you can connect to it using Chrome DevTools or another compatible debugger.

   Example:
   ```
   node --inspect app.js
   ```

3. **Debugger Statement**: You can use the `debugger` statement in your code to trigger a breakpoint. When Node.js encounters the `debugger` statement during execution, it will pause the execution and allow you to inspect the program's state using a debugger.

   Example:
   ```javascript
   function myFunction() {
       let x = 10;
       debugger;
       console.log(x);
   }
   myFunction();
   ```

4. **Node.js Debug Module**: Node.js also provides a built-in `debug` module that allows you to add conditional logging to your code. You can enable debug output by setting the `DEBUG` environment variable to the name of your module.

   Example:
   ```javascript
   const debug = require('debug')('myapp');

   function myFunction() {
       debug('Entering myFunction');
       let x = 10;
       debug('Value of x:', x);
       console.log(x);
   }

   myFunction();
   ```

5. **Third-Party Debugging Tools**: There are many third-party debugging tools available for Node.js, such as Visual Studio Code, WebStorm, and JetBrains' Node.js debugger. These tools provide advanced debugging features such as breakpoints, step-through execution, variable inspection, and call stack analysis.

6. **Error Handling**: Proper error handling using try-catch blocks, error events, or error middleware can help identify and handle runtime errors in your application.

By using these debugging techniques and tools, you can effectively diagnose and fix issues in your Node.js applications during development and testing.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the concept of streams in Node.js.***

In Node.js, streams are objects that allow you to read or write data continuously, piece by piece, instead of loading the entire data into memory at once. Streams are a fundamental concept in Node.js for handling large amounts of data efficiently, such as reading from or writing to files, processing HTTP requests or responses, and transferring data over networks.

Streams in Node.js are implemented using EventEmitter, and they are instances of EventEmitter classes. There are four types of streams in Node.js:

1. **Readable streams**: Readable streams allow you to read data from a source, such as a file, an HTTP request, or a network socket. They provide a mechanism for reading data chunk by chunk as it becomes available, rather than loading the entire data into memory at once. Examples of readable streams include `fs.createReadStream()` for reading files and `http.IncomingMessage` for handling HTTP request bodies.

2. **Writable streams**: Writable streams allow you to write data to a destination, such as a file, an HTTP response, or a network socket. They provide a mechanism for writing data chunk by chunk, allowing you to efficiently handle large volumes of data. Examples of writable streams include `fs.createWriteStream()` for writing files and `http.ServerResponse` for sending HTTP responses.

3. **Duplex streams**: Duplex streams represent streams that can both read from and write to a source or destination. They combine the functionality of both readable and writable streams, allowing bidirectional data flow. Examples of duplex streams include network sockets and `process.stdin`/`process.stdout`.

4. **Transform streams**: Transform streams are a special type of duplex stream that allows you to modify or transform data as it is being read from a source and written to a destination. They provide a mechanism for performing data transformation operations, such as compression, encryption, or data manipulation. Examples of transform streams include `zlib.createGzip()` for compressing data and `crypto.createCipher()` for encrypting data.

Streams offer several benefits in Node.js, including:

- **Efficiency**: Streams allow you to process large amounts of data efficiently, without loading the entire data into memory at once. This makes them ideal for handling large files or network data.

- **Piping**: Streams can be easily connected together using piping (`readableStream.pipe(writableStream)`), allowing data to flow from one stream to another seamlessly. This simplifies data processing and reduces memory usage.

- **Event-based**: Streams are event-based and integrate seamlessly with Node.js's event-driven architecture. You can listen for events such as `data`, `end`, `error`, and `close` to handle stream-related events and perform appropriate actions.

Overall, streams are a powerful and versatile feature in Node.js that enable efficient handling of data flow in various I/O operations. They are essential for building high-performance and scalable applications in Node.js.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is RESTful API and how would you implement it in Node.js using Express?***

A RESTful API (Representational State Transfer) is an architectural style for designing networked applications. It is based on a set of principles and constraints that define how resources are identified and addressed, and how they can be manipulated using a uniform and stateless interface. RESTful APIs typically use HTTP methods (such as GET, POST, PUT, DELETE) to perform CRUD (Create, Read, Update, Delete) operations on resources, and they use standard HTTP status codes to indicate the outcome of the operation.

To implement a RESTful API in Node.js using Express, you can follow these steps:

1. **Set up Express**: First, you need to create a new Node.js project and install Express as a dependency using npm or yarn.

   ```
   npm install express
   ```

2. **Create Express Application**: Create a new Express application and set up basic configuration and middleware.

   ```javascript
   const express = require('express');
   const app = express();
   const port = 3000;

   app.use(express.json()); // Parse JSON request bodies

   // Define routes
   app.get('/api/users', (req, res) => {
       // Logic to fetch all users from the database
       res.json(users);
   });

   // Add more routes for other CRUD operations

   app.listen(port, () => {
       console.log(`Server is listening at http://localhost:${port}`);
   });
   ```

3. **Define Routes**: Define routes for different resources and HTTP methods. Each route should correspond to a specific resource and HTTP method.

   ```javascript
   // GET route to fetch a specific user by ID
   app.get('/api/users/:id', (req, res) => {
       const userId = req.params.id;
       // Logic to fetch user by ID from the database
       res.json(user);
   });

   // POST route to create a new user
   app.post('/api/users', (req, res) => {
       const newUser = req.body;
       // Logic to create a new user in the database
       res.status(201).json(newUser);
   });

   // PUT route to update an existing user
   app.put('/api/users/:id', (req, res) => {
       const userId = req.params.id;
       const updatedUser = req.body;
       // Logic to update user by ID in the database
       res.json(updatedUser);
   });

   // DELETE route to delete an existing user
   app.delete('/api/users/:id', (req, res) => {
       const userId = req.params.id;
       // Logic to delete user by ID from the database
       res.sendStatus(204);
   });
   ```

4. **Implement Controller Logic**: Implement the logic for handling requests and interacting with the database or other data sources inside route handlers.

5. **Handle Responses**: Handle responses appropriately by sending back JSON data, status codes, and error messages as needed.

6. **Test the API**: Test the API using tools like Postman, cURL, or your browser to ensure that it behaves as expected and handles requests correctly.

By following these steps, you can implement a basic RESTful API in Node.js using Express. Remember to follow RESTful principles such as using appropriate HTTP methods, status codes, and resource URLs to design a well-structured and intuitive API.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do you handle form data submission in Node.js?***

To handle form data submission in Node.js, especially with Express, you can use middleware such as `body-parser` or `multer` to parse the form data from incoming HTTP requests. Here's a basic example of how to handle form data submission with Express and `body-parser`:

1. Install `body-parser` module:

```bash
npm install body-parser
```

2. Require `body-parser` in your Node.js/Express application:

```javascript
const express = require('express');
const bodyParser = require('body-parser');

const app = express();
const port = 3000;

// Parse URL-encoded data from incoming requests
app.use(bodyParser.urlencoded({ extended: false }));

// Parse JSON data from incoming requests
app.use(bodyParser.json());

// Your route handlers go here...
```

3. Handle form submission in your route handler:

```javascript
app.post('/submit-form', (req, res) => {
  const formData = req.body;

  // Access form fields
  const name = formData.name;
  const email = formData.email;
  //...

  // Process the form data
  //...

  res.send('Form submitted successfully!');
});
```

4. Create a form in your HTML file:

```html
<form action="/submit-form" method="POST">
  <label for="name">Name:</label>
  <input type="text" id="name" name="name">

  <label for="email">Email:</label>
  <input type="email" id="email" name="email">

  <!-- Add more form fields as needed -->

  <button type="submit">Submit</button>
</form>
```

In this example:

- We're using `body-parser` middleware to parse form data (`application/x-www-form-urlencoded` and `application/json`) from incoming requests.
- When the form is submitted, the data is sent as a POST request to the `/submit-form` endpoint.
- In the route handler for `/submit-form`, we access the form data using `req.body`.
- You can then process the form data as needed and send a response back to the client.

This is a basic example of handling form data submission in Node.js with Express. If your form includes file uploads, you would need to use a different middleware such as `multer` to handle multipart form data.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is CORS and how do you handle it in a Node.js application?***

CORS (Cross-Origin Resource Sharing) is a security feature implemented by web browsers to restrict cross-origin HTTP requests initiated from scripts running in the browser. It's a mechanism that allows servers to specify which origins are permitted to access the resources on the server.

When a web application hosted on one domain (origin) tries to make an HTTP request to a different domain, the browser typically blocks the request due to the same-origin policy, which is a security measure to prevent malicious scripts from accessing resources from other origins. CORS provides a way for servers to relax this restriction and explicitly allow cross-origin requests.

In a Node.js application, you can handle CORS using middleware such as `cors` or by implementing custom middleware. Here's how to handle CORS using the `cors` middleware:

1. Install the `cors` package:

```bash
npm install cors
```

2. Use the `cors` middleware in your Express application:

```javascript
const express = require('express');
const cors = require('cors');

const app = express();
const port = 3000;

// Enable CORS for all routes
app.use(cors());

// Define your routes...
```

By using `app.use(cors())`, you enable CORS for all routes in your Express application. This allows requests from any origin to access your server's resources. You can also configure `cors` with options to specify which origins, methods, and headers are allowed.

```javascript
const corsOptions = {
  origin: 'http://example.com', // Allow requests from this origin
  methods: ['GET', 'POST'], // Allow these HTTP methods
  allowedHeaders: ['Content-Type', 'Authorization'], // Allow these headers
};

app.use(cors(corsOptions));
```

Alternatively, if you prefer to implement custom CORS middleware, you can define a middleware function to add the necessary CORS headers to the HTTP responses:

```javascript
app.use((req, res, next) => {
  res.setHeader('Access-Control-Allow-Origin', 'http://example.com');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');
  next();
});
```

With custom middleware, you have more control over the CORS configuration and can customize it based on your specific requirements.

Handling CORS is crucial when building web applications that interact with APIs hosted on different domains. By properly configuring CORS, you can ensure that your application can securely make cross-origin requests while still protecting against unauthorized access to your server's resources.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the difference between require() and import in Node.js.***

In Node.js, `require()` and `import` are both used to include external modules or files in a JavaScript file, but they have some differences in terms of syntax, behavior, and compatibility.

1. **`require()`**:
   - `require()` is the CommonJS module system used in Node.js for importing modules.
   - It is a synchronous function that loads modules dynamically at runtime.
   - `require()` is typically used to import modules that are included in the Node.js ecosystem or modules that use CommonJS syntax.
   - It is the standard way of importing modules in Node.js.
   - Example:
     ```javascript
     const fs = require('fs');
     const http = require('http');
     ```

2. **`import`**:
   - `import` is the ECMAScript module system introduced in ES6 (ES2015) for importing modules in JavaScript.
   - It is a declarative syntax for importing modules, and it supports both synchronous and asynchronous module loading.
   - `import` statements are always hoisted to the top of the file, meaning they are executed before any other code in the file.
   - `import` statements support static analysis, which allows tools to optimize module loading and tree-shake unused exports.
   - `import` statements are not yet fully supported in Node.js without using additional tools or transpilers like Babel.
   - Example:
     ```javascript
     import fs from 'fs';
     import http from 'http';
     ```

In summary, `require()` is the traditional way of importing modules in Node.js and is used with CommonJS modules, while `import` is the modern syntax introduced in ES6 for importing modules in JavaScript and is used with ECMAScript modules. While Node.js has started to support ECMAScript modules (`import` syntax) in recent versions, it's important to note that full support may still require additional configuration or the use of transpilers like Babel.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is event-driven programming in Node.js? How does it differ from traditional programming?***

Event-driven programming in Node.js is a paradigm where the flow of the program is determined by events such as user actions, system events, or messages from other programs rather than being strictly sequential. In this paradigm, the program listens for events and triggers appropriate callbacks or handlers when those events occur. Node.js is particularly well-suited for event-driven programming because it is built on a non-blocking, asynchronous architecture.

Here's how event-driven programming in Node.js differs from traditional programming paradigms:

1. **Asynchronous I/O**: In traditional programming, I/O operations (such as reading from a file or making a network request) are typically blocking, meaning the program waits for the operation to complete before continuing. In Node.js, I/O operations are non-blocking, allowing the program to continue executing other tasks while waiting for I/O operations to complete. This is achieved through the use of callbacks, Promises, or async/await syntax.

2. **Single-threaded, Event Loop**: Node.js operates on a single-threaded event loop, which means it can handle multiple concurrent operations without spawning additional threads. This is in contrast to traditional multi-threaded programming where each task may run on its own thread. The event loop continuously listens for events and dispatches them to event handlers, allowing Node.js to efficiently handle large numbers of concurrent connections.

3. **Event Emitters**: In Node.js, many objects are event emitters, meaning they can emit named events that cause listeners (callbacks) to be invoked. This enables a reactive style of programming where actions are triggered by events rather than explicitly called by the program.

4. **Callbacks and Asynchronous Control Flow**: In event-driven programming with Node.js, callbacks are commonly used to handle asynchronous operations. Instead of blocking and waiting for an operation to complete, a callback function is passed to an asynchronous function, which is then invoked when the operation completes. This allows the program to continue executing other tasks while waiting for the asynchronous operation to finish.

Overall, event-driven programming in Node.js offers scalability and performance benefits, particularly for applications that require high concurrency and responsiveness, such as web servers and real-time applications. However, it also requires a different mindset and programming style compared to traditional sequential programming paradigms.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the differences between process.nextTick() and setImmediate()? When would you use each?***

`process.nextTick()` and `setImmediate()` are both functions in Node.js that allow you to schedule asynchronous code execution, but they have different behaviors and use cases:

1. **`process.nextTick()`**:
   - `process.nextTick()` schedules a callback to be invoked in the next iteration of the event loop, immediately after the current operation completes and before any I/O events are triggered.
   - The callback passed to `process.nextTick()` is executed before any other I/O events, timers, or `setImmediate()` callbacks.
   - It is executed in the same phase of the event loop as the calling code.
   - Use `process.nextTick()` when you want to ensure that a callback is executed asynchronously but as soon as possible, even before any I/O events.

Example:
```javascript
function foo() {
    console.log('foo');
}

process.nextTick(foo);

console.log('bar');
```
Output:
```
bar
foo
```

2. **`setImmediate()`**:
   - `setImmediate()` schedules a callback to be invoked in the next iteration of the event loop, immediately after the current operation completes and after any I/O events that are already in the event queue.
   - It provides a mechanism to execute code asynchronously but without blocking I/O operations.
   - `setImmediate()` callbacks are executed in a separate phase of the event loop from `process.nextTick()` callbacks and timers.
   - Use `setImmediate()` when you want to ensure that a callback is executed asynchronously but after I/O events have been processed.

Example:
```javascript
function foo() {
    console.log('foo');
}

setImmediate(foo);

console.log('bar');
```
Output:
```
bar
foo
```

In summary:
- Use `process.nextTick()` when you want to execute a callback asynchronously but as soon as possible, even before I/O events.
- Use `setImmediate()` when you want to execute a callback asynchronously but after I/O events have been processed.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is callback hell and how do you avoid it in Node.js?***

Callback hell, also known as "Pyramid of Doom," refers to the situation in asynchronous programming where multiple nested callbacks are used to handle asynchronous operations, leading to code that is difficult to read, understand, and maintain. This typically occurs when dealing with asynchronous tasks such as file I/O, network requests, or database operations in Node.js.

Here's an example of callback hell:

```javascript
asyncOperation1((err, result1) => {
    if (err) {
        console.error(err);
    } else {
        asyncOperation2(result1, (err, result2) => {
            if (err) {
                console.error(err);
            } else {
                asyncOperation3(result2, (err, result3) => {
                    if (err) {
                        console.error(err);
                    } else {
                        // Handle result3
                    }
                });
            }
        });
    }
});
```

Asynchronous operations are nested inside each other, making the code difficult to follow and manage, especially as more operations are added.

To avoid callback hell and improve code readability in Node.js, you can use various techniques:

1. **Named Functions**: Define named functions for callback operations to make the code more modular and readable. This separates the concerns and reduces nesting.

```javascript
asyncOperation1((err, result1) => {
    if (err) {
        console.error(err);
    } else {
        asyncOperation2(result1, handleOperation2);
    }
});

function handleOperation2(err, result2) {
    if (err) {
        console.error(err);
    } else {
        asyncOperation3(result2, handleOperation3);
    }
}

function handleOperation3(err, result3) {
    if (err) {
        console.error(err);
    } else {
        // Handle result3
    }
}
```

2. **Promises**: Use Promises to handle asynchronous operations sequentially or in parallel, and chain them together using `.then()` and `.catch()`.

```javascript
asyncOperation1()
    .then(result1 => asyncOperation2(result1))
    .then(result2 => asyncOperation3(result2))
    .then(result3 => {
        // Handle result3
    })
    .catch(err => {
        console.error(err);
    });
```

3. **Async/Await**: Use `async` and `await` keywords to write asynchronous code in a synchronous style, making it easier to read and understand.

```javascript
try {
    const result1 = await asyncOperation1();
    const result2 = await asyncOperation2(result1);
    const result3 = await asyncOperation3(result2);
    // Handle result3
} catch (err) {
    console.error(err);
}
```

By using these techniques, you can avoid callback hell and write more maintainable, readable, and scalable asynchronous code in Node.js.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is the role of EventEmitter in Node.js? Can you provide an example of how to use it?***

The `EventEmitter` class in Node.js is a core module that provides an implementation of the observer pattern. It allows objects (known as "emitters") to emit named events that cause function callbacks (known as "listeners") to be invoked. This enables a reactive style of programming where actions are triggered by events rather than explicitly called by the program.

Here's a brief overview of the roles and features of `EventEmitter`:

1. **Emitting Events**: An emitter object can emit events using the `emit()` method, specifying the event name and optional data to pass to listeners.

2. **Registering Event Listeners**: You can register listeners for specific events using the `on()` method (or `addListener()` method), providing the event name and a callback function to be invoked when the event occurs.

3. **Removing Event Listeners**: You can remove specific listeners for an event using the `removeListener()` method, or remove all listeners for an event using the `removeAllListeners()` method.

4. **Error Handling**: `EventEmitter` provides a special event name, `'error'`, which is triggered when an error occurs during event emission if no listeners are registered for the `'error'` event, the error will be thrown, causing the Node.js process to terminate.

Here's a simple example demonstrating how to use `EventEmitter` in Node.js:

```javascript
const EventEmitter = require('events');

// Create a new EventEmitter instance
const myEmitter = new EventEmitter();

// Register a listener for the 'greet' event
myEmitter.on('greet', (name) => {
    console.log(`Hello, ${name}!`);
});

// Emit the 'greet' event with a parameter
myEmitter.emit('greet', 'John');
```

In this example:

- We create a new instance of `EventEmitter` called `myEmitter`.
- We register a listener for the `'greet'` event using the `on()` method. When the `'greet'` event is emitted, the provided callback function will be invoked with the specified `name` parameter.
- We emit the `'greet'` event with the parameter `'John'`. This triggers the execution of the listener callback function, which logs `'Hello, John!'` to the console.

This is a basic illustration of how `EventEmitter` works in Node.js. It's a powerful tool for building event-driven applications, such as web servers, real-time messaging systems, and more.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the differences between fork, spawn, and exec in Node.js? When would you use each?***

In Node.js, `fork`, `spawn`, and `exec` are different methods for creating child processes, each with its own purpose and use cases:

1. **`fork`**:
   - The `fork` method is a specialized form of `spawn` used specifically for creating new Node.js processes. It's commonly used to run separate instances of a Node.js application, each with its own memory space and communication channel via inter-process communication (IPC).
   - `fork` is ideal for creating multiple instances of a Node.js application, such as in a cluster setup, where each instance can handle incoming requests independently.
   - Communication between the parent and child processes created by `fork` is simplified using the built-in `send()` and `message` events.

2. **`spawn`**:
   - The `spawn` method is a general-purpose method for spawning new processes in Node.js. It allows you to execute any command in a new child process.
   - `spawn` is commonly used for running non-Node.js commands or executables, such as system utilities or shell commands.
   - Unlike `exec`, `spawn` does not create a shell to execute the command, which makes it more efficient and secure. You can specify the command and its arguments directly as an array.
   - You can also stream data between the parent and child processes using stdin, stdout, and stderr.

3. **`exec`**:
   - The `exec` method is another way to spawn new processes in Node.js, specifically designed for running shell commands.
   - `exec` creates a shell to execute the specified command, allowing you to use shell features such as pipes, redirection, and chaining commands with `&&` or `|`.
   - It's useful when you need to execute complex shell commands or when you want to take advantage of shell features.
   - `exec` buffers the command's stdout and stderr output, making it suitable for commands that produce a moderate amount of output. However, this buffering can cause issues with commands that produce a large amount of output.

Here's a summary of when to use each method:
- **Use `fork`** when you need to create multiple instances of a Node.js application and want to facilitate communication between the parent and child processes using IPC.
- **Use `spawn`** when you need to execute non-Node.js commands or executables, and you want direct control over input/output streams without involving a shell.
- **Use `exec`** when you need to execute shell commands and want to take advantage of shell features, but be cautious with commands that produce large output.

Overall, the choice between `fork`, `spawn`, and `exec` depends on your specific requirements, including the type of command you need to execute, the level of control you need over input/output, and the desired communication mechanism between parent and child processes.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the security best practices to follow when developing a Node.js application?***

Developing a secure Node.js application involves implementing various best practices across different aspects of the application, including code, dependencies, configuration, authentication, and data handling. Here are some security best practices to follow:

1. **Keep Dependencies Updated**: Regularly update dependencies to patch known vulnerabilities. Use tools like `npm audit` to identify and fix vulnerable packages.

2. **Secure Authentication**:
   - Use strong cryptographic algorithms for password hashing, such as bcrypt or Argon2.
   - Implement multi-factor authentication (MFA) wherever possible.
   - Store sensitive data like passwords and API keys securely, preferably using environment variables or a secure secrets management system.

3. **Enable HTTPS**: Always use HTTPS to encrypt data transmitted between the client and server. Obtain and configure SSL/TLS certificates from trusted certificate authorities.

4. **Input Validation and Sanitization**:
   - Validate and sanitize all user inputs to prevent injection attacks like SQL injection, XSS, and command injection.
   - Use parameterized queries or ORM libraries to prevent SQL injection attacks in database queries.

5. **Prevent Cross-Site Scripting (XSS)**: Sanitize user-generated content and escape HTML entities to prevent XSS attacks.

6. **Implement CSRF Protection**: Use CSRF tokens and enforce same-origin policy to prevent Cross-Site Request Forgery attacks.

7. **Secure Session Management**:
   - Use secure, HTTP-only cookies for session management.
   - Implement session expiration and rotation mechanisms.
   - Store session data securely, avoiding client-side storage for sensitive information.

8. **Implement Access Controls**: Enforce proper access controls and authorization mechanisms to restrict access to sensitive resources based on user roles and permissions.

9. **Enable Security Headers**: Set security headers like Content Security Policy (CSP), X-Frame-Options, X-XSS-Protection, and X-Content-Type-Options to mitigate various web vulnerabilities.

10. **Secure File Uploads**: Validate file types, size, and content during file uploads to prevent file-based attacks like file inclusion, file overwrite, and executable file uploads.

11. **Avoid Eval and Unsafe Functions**: Avoid using `eval`, `Function()`, `setTimeout()` with string arguments, or any other functions that execute arbitrary code from user input, as they can introduce security vulnerabilities.

12. **Logging and Monitoring**: Implement comprehensive logging and monitoring to detect and respond to security incidents in real-time. Monitor for unusual activities and potential security threats.

13. **Regular Security Audits and Penetration Testing**: Conduct regular security audits and penetration testing to identify vulnerabilities and weaknesses in your application. Address any findings promptly.

14. **Educate Developers and Users**: Educate developers on secure coding practices and conduct security awareness training for users to prevent social engineering attacks and improve overall security posture.

15. **Follow Security Standards and Guidelines**: Adhere to industry-standard security frameworks and guidelines, such as OWASP Top 10, CIS Benchmarks, and Node.js Security Checklist, to ensure comprehensive security coverage.

By following these security best practices, you can significantly reduce the risk of security vulnerabilities and protect your Node.js application from various threats and attacks.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is clustering in Node.js and how does it improve performance?***

Clustering in Node.js refers to the ability of the Node.js runtime to spawn multiple instances of a single Node.js process, known as worker processes, which can share the same port and efficiently handle incoming requests. This is particularly useful for applications that need to scale and handle a large number of concurrent connections, such as web servers.

Here's how clustering works and how it improves performance:

1. **Master-Worker Architecture**: In clustering, there is a master process that manages the worker processes. The master process listens for incoming connections and distributes them among the worker processes in a round-robin fashion. Each worker process operates independently, handling its own set of requests.

2. **Improved Concurrency**: By distributing incoming requests among multiple worker processes, clustering allows Node.js applications to utilize multiple CPU cores efficiently. Each worker process runs on a separate CPU core, enabling parallel processing of requests and better utilization of system resources.

3. **Increased Throughput**: With clustering, a Node.js application can handle a higher volume of concurrent connections compared to a single-threaded or single-process setup. This leads to improved throughput and better response times, especially under heavy load conditions.

4. **High Availability**: Clustering also improves the resilience and availability of Node.js applications. If a worker process crashes due to an error, the master process can restart it automatically, ensuring uninterrupted service for incoming requests.

5. **Horizontal Scaling**: Clustering facilitates horizontal scaling of Node.js applications by adding more instances (worker processes) as needed to handle increasing traffic and workload. This allows applications to scale dynamically based on demand without requiring significant changes to the codebase.

6. **Load Balancing**: Clustering can be combined with load balancing techniques to further distribute incoming traffic across multiple instances of a Node.js application. Load balancers can distribute requests based on various criteria, such as round-robin, least connections, or server health, to ensure optimal performance and resource utilization.

Overall, clustering in Node.js enables applications to achieve higher performance, scalability, and availability by leveraging the capabilities of modern multi-core processors and efficiently distributing incoming requests among multiple worker processes. It's a key feature for building high-performance web servers and other network-intensive applications in Node.js.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is JWT authentication and how would you implement it in a Node.js application?***

JWT (JSON Web Token) authentication is a method of securely transmitting information between parties as a JSON object. It's commonly used for implementing stateless authentication mechanisms in web applications. In JWT authentication, a token is generated by the server upon successful authentication of a user. This token contains encoded information (such as user ID, expiration time, and any other relevant data) and is digitally signed using a secret key known only to the server. The client then includes this token in subsequent requests to access protected resources.

Here's how you can implement JWT authentication in a Node.js application:

1. **Install Dependencies**: First, install the necessary packages using npm or yarn.

```bash
npm install jsonwebtoken bcrypt
```

2. **Generate JWT Token**: When a user successfully logs in, generate a JWT token containing relevant user information.

```javascript
const jwt = require('jsonwebtoken');

// Generate JWT token
const generateToken = (user) => {
    const token = jwt.sign({ id: user.id, email: user.email }, 'your_secret_key', { expiresIn: '1h' });
    return token;
};
```

3. **Verify JWT Token**: Create middleware to verify and decode the JWT token included in the request headers.

```javascript
const verifyToken = (req, res, next) => {
    const token = req.headers.authorization;
    if (!token) {
        return res.status(403).json({ message: 'Token not provided' });
    }

    jwt.verify(token, 'your_secret_key', (err, decoded) => {
        if (err) {
            return res.status(401).json({ message: 'Failed to authenticate token' });
        }
        req.user = decoded;
        next();
    });
};
```

4. **Protect Routes**: Apply the `verifyToken` middleware to the routes that need authentication.

```javascript
const express = require('express');
const app = express();

// Protect route with JWT authentication
app.get('/protected', verifyToken, (req, res) => {
    res.json({ message: 'Authenticated route', user: req.user });
});
```

5. **Client-side Implementation**: Include the JWT token in the Authorization header for subsequent requests.

```javascript
const axios = require('axios');

// Example: Making a request with JWT token
const token = 'Bearer ' + localStorage.getItem('token');

axios.get('/protected', { headers: { Authorization: token } })
    .then(response => {
        console.log(response.data);
    })
    .catch(error => {
        console.error(error);
    });
```

6. **Optional: Hash Passwords**: When storing user passwords, always hash them using a strong hashing algorithm like bcrypt to ensure security.

```javascript
const bcrypt = require('bcrypt');

// Hash password
const hashPassword = async (password) => {
    const salt = await bcrypt.genSalt(10);
    const hashedPassword = await bcrypt.hash(password, salt);
    return hashedPassword;
};
```

This implementation provides a basic example of JWT authentication in a Node.js application. Remember to securely store the secret key used for signing and verifying JWT tokens, and always use HTTPS to protect against token interception. Additionally, consider implementing token expiration and token refresh mechanisms for improved security.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do you handle file uploads in Node.js?***

Handling file uploads in Node.js involves receiving files sent from clients via HTTP requests, processing them, and saving them to a storage location. Here's a basic example of how to handle file uploads using Express.js, a popular web framework for Node.js:

1. **Install Dependencies**: First, install the necessary packages using npm or yarn.

```bash
npm install express multer
```

2. **Set Up Express Server**: Create an Express server and configure middleware to handle file uploads using Multer, a middleware for handling multipart/form-data.

```javascript
const express = require('express');
const multer = require('multer');
const app = express();

// Set up Multer middleware
const storage = multer.diskStorage({
    destination: function (req, file, cb) {
        cb(null, 'uploads/'); // Save uploaded files to the 'uploads' directory
    },
    filename: function (req, file, cb) {
        cb(null, Date.now() + '-' + file.originalname); // Generate unique filename
    }
});

const upload = multer({ storage: storage });

// Define route to handle file upload
app.post('/upload', upload.single('file'), (req, res) => {
    res.json({ message: 'File uploaded successfully', filename: req.file.filename });
});

// Start server
const PORT = 3000;
app.listen(PORT, () => {
    console.log(`Server running on port ${PORT}`);
});
```

3. **Client-side Implementation**: Create a form in your HTML file to allow users to select and upload files.

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>File Upload</title>
</head>
<body>
    <form action="/upload" method="POST" enctype="multipart/form-data">
        <input type="file" name="file">
        <button type="submit">Upload</button>
    </form>
</body>
</html>
```

4. **Handle File Upload**: When the form is submitted, the file will be uploaded to the server. Multer middleware handles the file upload process, including saving the file to the specified destination directory (`uploads/` in this example) with a unique filename.

5. **Process Uploaded Files**: After the file is uploaded, you can process it as needed. In this example, the server responds with a JSON object containing a message indicating that the file was uploaded successfully and the filename.

This is a basic example of handling file uploads in Node.js using Express.js and Multer. Depending on your requirements, you may need to add additional validation, error handling, and security measures, such as file type checking, file size limits, and sanitization to prevent attacks like directory traversal.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Difference between package.json and package-lock.json***

`package.json` and `package-lock.json` are both files used in Node.js projects to manage dependencies, but they serve different purposes and have different formats.

1. **package.json**:
   - `package.json` is a metadata file for Node.js projects that contains information about the project, such as its name, version, description, dependencies, scripts, and other metadata.
   - It is manually maintained by developers and serves as a manifest for the project, specifying the project's dependencies and other configuration settings.
   - Developers typically add dependencies to `package.json` using `npm install` or `yarn add` commands, which automatically updates the `dependencies` or `devDependencies` section of the file.
   - `package.json` is used for dependency management, project configuration, and script execution.

2. **package-lock.json**:
   - `package-lock.json` is a lockfile automatically generated by npm when installing dependencies. Yarn uses a similar file named `yarn.lock`.
   - It serves as a record of the exact versions of dependencies (and their sub-dependencies) installed in the project tree.
   - The purpose of `package-lock.json` is to provide deterministic and reproducible builds by ensuring that subsequent installations of dependencies will use the same versions as those specified in the lockfile.
   - It includes detailed information about each dependency, such as its version, resolved URL, and integrity checksum, to ensure consistency across different installations and environments.
   - `package-lock.json` should not be manually modified; it is meant to be automatically generated and maintained by npm or Yarn.

In summary, `package.json` is a project manifest file where developers specify project metadata and dependencies, while `package-lock.json` is an automatically generated lockfile that ensures deterministic dependency resolution and reproducible builds by recording the exact versions of dependencies installed. Both files are essential for managing dependencies and ensuring consistency and stability in Node.js projects.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***When should you npm and when yarn?***

Both npm and Yarn are package managers for Node.js projects, and they offer similar functionality for managing dependencies, scripts, and project configuration. However, there are some differences between the two, and the choice between npm and Yarn depends on factors such as performance, features, and personal preference.

Here are some considerations to help you decide when to use npm and when to use Yarn:

Use npm when:
1. **Default Package Manager**: npm is the default package manager for Node.js and comes bundled with Node.js installation, so it's readily available without additional setup.
2. **Stability and Maturity**: npm has been around for a longer time and is widely used in the Node.js ecosystem, making it a stable and mature choice for managing dependencies.
3. **Built-in Scripts**: npm has built-in support for running scripts defined in the `package.json` file, making it easy to define and execute custom commands.
4. **Compatibility**: npm is compatible with various Node.js tools and libraries and integrates well with other npm-based workflows and services.
5. **Simplicity**: For simple projects or those that don't require advanced features, npm may be preferable due to its simplicity and familiarity.

Use Yarn when:
1. **Performance**: Yarn is known for its faster and more efficient dependency resolution and installation compared to npm, especially for large projects with many dependencies. Yarn's caching mechanism and parallel installation capabilities contribute to its improved performance.
2. **Deterministic Installs**: Yarn generates a lockfile (`yarn.lock`) by default, which ensures deterministic dependency resolution and reproducible builds across different environments. This can help prevent unexpected dependency conflicts and ensure consistency in project dependencies.
3. **Offline Mode**: Yarn has built-in support for offline mode, allowing you to install dependencies without an internet connection if the packages are already cached locally. This is useful for development in environments with limited or no internet access.
4. **Security**: Yarn provides features like checksum verification and integration with security advisories to help identify and mitigate security vulnerabilities in dependencies.
5. **Workspaces**: Yarn supports workspaces, allowing you to manage multiple related packages within a single repository more efficiently. This is beneficial for monorepo setups and projects with multiple packages.

Ultimately, the choice between npm and Yarn depends on your specific requirements, preferences, and the needs of your project. Both package managers are capable of managing dependencies effectively, so you can choose the one that best fits your workflow and project goals. Additionally, it's worth noting that you can switch between npm and Yarn easily since they use the same `package.json` format and are largely compatible with each other.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is REPL? What purpose it is used for?***

REPL stands for "Read-Eval-Print Loop." It is an interactive programming environment that takes single user inputs (or expressions), evaluates them, prints the result, and then waits for the next input. The process repeats in a loop, hence the name "REPL."

Here's what each part of the acronym stands for:

1. **Read**: The REPL reads user input, typically a single line of code or expression.

2. **Eval**: The input is evaluated, meaning the code is executed or interpreted by the underlying programming language's runtime environment.

3. **Print**: The result of the evaluation is printed or displayed to the user.

4. **Loop**: After printing the result, the REPL loops back to the beginning, waiting for the next input from the user.

The REPL provides an interactive and immediate way to experiment with code, test ideas, and explore language features without the need to create and execute a separate script or program. It is particularly useful for learning a new programming language, debugging code, and prototyping algorithms.

In Node.js, you can access the REPL environment by running the `node` command without any arguments in your terminal or command prompt. This launches the Node.js REPL, where you can type JavaScript code and see the results immediately. You can exit the REPL by typing `.exit` or pressing Ctrl + C twice.

Here's an example of using the Node.js REPL:

```
$ node
> 2 + 3
5
> const greeting = 'Hello, world!'
undefined
> greeting
'Hello, world!'
> Math.sqrt(16)
4
> .exit
```

In this example, the user inputs expressions like addition, variable assignment, and function invocation, and the REPL evaluates them and prints the results accordingly.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is the purpose of the global object in Node.js?***

The global object in Node.js serves a similar purpose to the global object in browsers (e.g., `window` in browsers). It provides access to global variables and functions that are available throughout the Node.js application, regardless of the scope in which they are defined. The global object in Node.js is simply referred to as `global`.

Here are some key purposes and features of the global object in Node.js:

1. **Global Variables and Functions**: The global object provides access to built-in global variables and functions such as `console`, `process`, `Buffer`, `setTimeout`, `setInterval`, `clearTimeout`, `clearInterval`, etc. These variables and functions are available without the need for require statements and can be accessed from any module within the application.

2. **Global Scope**: Variables and functions defined in the top-level scope (outside of any function) in Node.js modules are implicitly added to the global object. This means that variables and functions defined in one module can be accessed from other modules via the global object.

3. **Global Namespace**: The global object serves as a namespace for variables and functions that need to be shared across different parts of the application. However, care should be taken to avoid polluting the global namespace excessively, as it can lead to naming conflicts and unintended side effects.

4. **Environment Information**: The global object provides access to environment-related information through properties such as `global.process` (equivalent to `process` module) and `global.console` (equivalent to `console` object).

5. **Custom Properties and Functions**: You can also add custom properties and functions to the global object as needed. However, this should be done sparingly and with caution to avoid conflicts with existing properties and functions.

While the global object in Node.js provides convenience for accessing commonly used variables and functions across modules, it's generally considered a best practice to minimize its usage and rely on module exports for encapsulating and sharing functionality between modules. This helps to promote modularity, maintainability, and code readability in Node.js applications.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***Explain the concept of child processes in Node.js. How would you create and manage them?***

In Node.js, child processes are separate instances of the Node.js runtime that can be spawned from the main (parent) Node.js process. Child processes allow Node.js applications to execute CPU-intensive or blocking operations asynchronously, leverage multiple CPU cores efficiently, and interact with other system processes or commands.

Here are the key concepts and methods for creating and managing child processes in Node.js:

1. **Spawning Child Processes**:
   - Node.js provides the `child_process` module, which includes several methods for spawning child processes, such as `spawn`, `exec`, `execFile`, and `fork`.
   - The `spawn` method is the most flexible and commonly used method. It launches a new process with the specified command, arguments, and options.
   - Example:
     ```javascript
     const { spawn } = require('child_process');
     const ls = spawn('ls', ['-lh', '/usr']);
     ```

2. **Interprocess Communication (IPC)**:
   - Child processes can communicate with the parent process using IPC channels.
   - By default, child processes inherit the standard input, output, and error streams of the parent process. However, you can create custom IPC channels using the `child.send()` method and the `message` event.
   - Example:
     ```javascript
     // Parent process
     const { fork } = require('child_process');
     const child = fork('child.js');
     child.send('Hello from parent');

     // Child process (child.js)
     process.on('message', (msg) => {
         console.log('Message from parent:', msg);
     });
     ```

3. **Handling Child Process Events**:
   - Child processes emit events that can be handled to monitor their lifecycle and status.
   - Common events include `exit`, `error`, and `close`.
   - Example:
     ```javascript
     child.on('exit', (code) => {
         console.log(`Child process exited with code ${code}`);
     });
     ```

4. **Managing Child Process Lifecycle**:
   - You can manage the lifecycle of child processes by monitoring their exit codes, terminating them using `kill()`, or gracefully shutting them down.
   - Example:
     ```javascript
     setTimeout(() => {
         child.kill('SIGTERM');
     }, 5000);
     ```

5. **Error Handling**:
   - It's important to handle errors when working with child processes to ensure robustness and reliability.
   - Example:
     ```javascript
     child.on('error', (err) => {
         console.error('Child process error:', err);
     });
     ```

By leveraging child processes, Node.js applications can improve performance, concurrency, and resource utilization, especially for tasks like parallel computation, file I/O, or executing external commands. However, it's essential to handle errors, manage resources efficiently, and ensure proper communication and coordination between parent and child processes to build reliable and scalable applications.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the different HTTP methods in Node.js? How would you handle each of them in an Express.js application?***

In Node.js, as in web development in general, HTTP methods are used to specify the action the client wants to perform on a resource. The most common HTTP methods are:

1. **GET**: The GET method is used to request data from a specified resource.

2. **POST**: The POST method is used to submit data to be processed to a specified resource.

3. **PUT**: The PUT method is used to update a resource or create a new resource if it does not exist.

4. **DELETE**: The DELETE method is used to delete a specified resource.

5. **PATCH**: The PATCH method is used to apply partial modifications to a resource.

6. **HEAD**: The HEAD method is similar to GET but only returns the headers and not the actual data.

7. **OPTIONS**: The OPTIONS method is used to describe the communication options for the target resource.

In an Express.js application, you can handle each of these HTTP methods using the corresponding methods provided by the Express router. Here's how you can handle each HTTP method in an Express.js application:

```javascript
const express = require('express');
const app = express();

// Handle GET requests
app.get('/', (req, res) => {
    res.send('GET request received');
});

// Handle POST requests
app.post('/', (req, res) => {
    res.send('POST request received');
});

// Handle PUT requests
app.put('/', (req, res) => {
    res.send('PUT request received');
});

// Handle DELETE requests
app.delete('/', (req, res) => {
    res.send('DELETE request received');
});

// Handle PATCH requests
app.patch('/', (req, res) => {
    res.send('PATCH request received');
});

// Handle HEAD requests
app.head('/', (req, res) => {
    res.send('HEAD request received');
});

// Handle OPTIONS requests
app.options('/', (req, res) => {
    res.send('OPTIONS request received');
});

// Start the server
const PORT = 3000;
app.listen(PORT, () => {
    console.log(`Server running on port ${PORT}`);
});
```

In this example:

- We use methods like `app.get()`, `app.post()`, `app.put()`, `app.delete()`, `app.patch()`, `app.head()`, and `app.options()` to define routes for handling different HTTP methods.
- Each route handler takes a path and a callback function as arguments. The callback function receives the request (`req`) and response (`res`) objects, allowing you to process the request and send a response.
- Inside each route handler, we use `res.send()` to send a simple response back to the client indicating that the request was received.

You can define more complex route handlers to perform operations like database queries, data manipulation, authentication, and more based on the HTTP method and request parameters.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is the role of module.exports and exports in Node.js modules? How do you use them?***

In Node.js, `module.exports` and `exports` are both used to define the public interface of a module and export values or functionality from one module to another. They are part of the CommonJS module system, which is the standard module system used in Node.js.

Here's an explanation of each and how to use them:

1. **module.exports**:
   - `module.exports` is an object that is created for every JavaScript module in Node.js. It is initially set to an empty object `{}`.
   - You can assign any value (such as an object, function, string, number, etc.) to `module.exports` to export it as the public interface of the module.
   - When you require the module in another file, the value assigned to `module.exports` will be returned.
   - Example:
     ```javascript
     // greet.js
     function greet(name) {
         return `Hello, ${name}!`;
     }

     module.exports = greet;
     ```

2. **exports**:
   - `exports` is a shorthand reference to `module.exports`. Initially, `exports` points to the same empty object `{}` as `module.exports`.
   - You can directly add properties or methods to `exports` to export them. However, you cannot reassign `exports` to a new value like you can with `module.exports`.
   - While you can use `exports` for simpler cases, it's recommended to use `module.exports` for more complex exports or when you need to export a single value or function.
   - Example:
     ```javascript
     // greet.js
     exports.greet = function(name) {
         return `Hello, ${name}!`;
     };
     ```

Here's how you can use these exports in another file:

```javascript
// main.js
const greet = require('./greet');

console.log(greet('John')); // Output: Hello, John!
```

In this example:
- In `greet.js`, we define a function (`greet`) and export it using `module.exports`.
- In `main.js`, we require the `greet` module using `require('./greet')`. The value exported by `module.exports` in `greet.js` is returned and assigned to the `greet` variable in `main.js`.
- We can then use the `greet` function in `main.js` as if it were defined locally.
  
<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What are the benefits of using TypeScript with Node.js? How would you set up a Node.js project with TypeScript?***

Using TypeScript with Node.js offers several benefits, including:

1. **Static Typing**: TypeScript adds static typing to JavaScript, allowing you to catch type-related errors during development. This leads to more robust code and reduces the likelihood of runtime errors.

2. **Enhanced IDE Support**: TypeScript provides better IntelliSense and code completion in editors like Visual Studio Code, helping developers write code faster and with fewer errors.

3. **Improved Maintainability**: TypeScript's type system makes code easier to understand and maintain, especially in larger projects with multiple contributors. Types serve as documentation and help developers understand the structure of the codebase.

4. **Modern JavaScript Features**: TypeScript supports modern ECMAScript features, even before they are fully supported in Node.js. This allows you to use features like async/await, destructuring, and arrow functions without worrying about compatibility issues.

5. **Better Tooling and Refactoring Support**: TypeScript's tooling support allows for better code refactoring, navigation, and code analysis, making it easier to work with large codebases and refactor existing code.

To set up a Node.js project with TypeScript, you can follow these steps:

1. **Initialize the project**: Create a new directory for your project and navigate to it in the terminal. Then, run `npm init` to initialize a new Node.js project. You can follow the prompts to set up your `package.json` file.

2. **Install TypeScript**: Install TypeScript as a development dependency using npm.

   ```bash
   npm install typescript --save-dev
   ```

3. **Create a tsconfig.json file**: Create a `tsconfig.json` file in the root of your project to configure TypeScript compiler options. You can generate a basic `tsconfig.json` file by running:

   ```bash
   npx tsc --init
   ```

   This command generates a `tsconfig.json` file with default settings. You can customize it according to your project requirements.

4. **Write TypeScript code**: Write your Node.js code in TypeScript files (`.ts` extension). You can start by creating an entry file (e.g., `index.ts`) and adding your code.

5. **Compile TypeScript to JavaScript**: Compile your TypeScript code to JavaScript using the TypeScript compiler (`tsc`). You can run the compiler in watch mode to automatically recompile files when they change:

   ```bash
   npx tsc --watch
   ```

6. **Run your Node.js application**: After compiling TypeScript files to JavaScript, you can run your Node.js application as usual:

   ```bash
   node dist/index.js
   ```

   (Assuming your compiled JavaScript files are in a `dist` directory.)

7. **Add TypeScript-specific packages (optional)**: Depending on your project needs, you may want to install additional packages for TypeScript support in Node.js, such as `@types/node` for type definitions for Node.js modules.

   ```bash
   npm install @types/node --save-dev
   ```

By following these steps, you can set up a Node.js project with TypeScript and take advantage of TypeScript's features and benefits for developing Node.js applications.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***What is the purpose of the __dirname and __filename variables in Node.js?***

In Node.js, `__dirname` and `__filename` are special variables that provide information about the current directory and the current file's absolute path, respectively.

1. **`__dirname`**:
   - `__dirname` contains the absolute path of the directory where the currently executing script resides.
   - It always refers to the directory of the current module, regardless of where it is called from.
   - `__dirname` does not include the name of the current file; it only represents the directory containing the file.
   - Example usage:
     ```javascript
     console.log(__dirname); // Output: /path/to/current/directory
     ```

2. **`__filename`**:
   - `__filename` contains the absolute path of the currently executing script (module file).
   - It represents the full path, including the filename, of the current module file.
   - `__filename` is useful for getting information about the location of the current module or for accessing the current module's content programmatically.
   - Example usage:
     ```javascript
     console.log(__filename); // Output: /path/to/current/directory/currentFile.js
     ```

Both `__dirname` and `__filename` are often used in Node.js applications for tasks such as:
- Resolving file paths relative to the current module's directory.
- Loading other modules or files dynamically using their absolute paths.
- Accessing resources or files located in the same directory as the current module.

These variables provide convenient ways to access file system-related information within Node.js modules, making it easier to work with files and directories in Node.js applications.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How would you handle environment variables in a Node.js application?***

Handling environment variables in a Node.js application is important for managing sensitive information such as API keys, database credentials, and configuration settings that may vary between different environments (e.g., development, staging, production). Here's how you can handle environment variables in a Node.js application:

1. **Use `process.env`**:
   - Node.js provides the `process.env` object, which is a property that exposes the system environment variables to the Node.js process.
   - You can access environment variables using `process.env.VARIABLE_NAME`, where `VARIABLE_NAME` is the name of the environment variable.
   - Example:
     ```javascript
     const apiKey = process.env.API_KEY;
     ```

2. **Set Environment Variables**:
   - Environment variables can be set in various ways, depending on the environment and deployment method:
     - Using a `.env` file: Create a `.env` file in the root of your project and define environment variables in the format `VARIABLE_NAME=value`. Use a package like `dotenv` to load the `.env` file and populate `process.env` automatically.
     - Using command-line arguments: Pass environment variables directly to the Node.js process using command-line arguments (e.g., `NODE_ENV=production node app.js`).
     - Using a hosting provider: Many hosting providers allow you to configure environment variables through their dashboard or command-line interface.

3. **Fallback Values**:
   - Provide fallback values or default configurations for environment variables to ensure that your application behaves correctly even if some variables are not defined.
   - Example:
     ```javascript
     const port = process.env.PORT || 3000;
     ```

4. **Avoid Hardcoding Sensitive Information**:
   - Avoid hardcoding sensitive information like API keys and passwords directly into your code. Instead, store them as environment variables and access them using `process.env`.
   - This helps keep sensitive information out of your codebase and reduces the risk of exposing it accidentally.

5. **Protect Environment Variables**:
   - Treat environment variables containing sensitive information with care and avoid exposing them unnecessarily.
   - Use tools like `.gitignore` to exclude `.env` files from version control and configure deployment environments securely.

6. **Configuration Management**:
   - For more complex applications, consider using a configuration management solution or library (e.g., `config`, `nconf`) to manage environment variables, load configuration files, and provide a structured way to access configuration settings.

By following these practices, you can effectively handle environment variables in your Node.js application, ensuring security, portability, and configurability across different environments.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***what are buffer and why we need to use with node js***

In the context of Node.js, a buffer is essentially a temporary storage space in memory. It's a data structure that allows you to work with binary data directly in JavaScript. Buffers are particularly useful when dealing with binary data, such as reading from or writing to streams, handling file uploads, or working with network protocols.

Here's why buffers are commonly used with Node.js:

1. **Efficient Handling of Binary Data**: JavaScript's native data types are not well-suited for handling binary data efficiently. Buffers provide a way to efficiently manipulate binary data directly in memory, making tasks like reading from files or network sockets much faster and more straightforward.

2. **File Operations**: Buffers are commonly used when reading from or writing to files. When you read data from a file using Node.js file system APIs, it's typically returned as a buffer. Similarly, when you write data to a file, you often provide a buffer containing the data to be written.

3. **Network Operations**: Buffers are also essential for working with network operations, such as sending and receiving data over sockets. When you receive data from a socket, it's usually stored in a buffer. Likewise, when you send data over a socket, you typically provide a buffer containing the data to be sent.

4. **Data Manipulation**: Buffers provide a set of methods for manipulating binary data, such as copying, slicing, and converting between different encodings. These methods make it easier to work with binary data in Node.js applications.

Overall, buffers are an essential part of Node.js, especially when working with binary data or performing I/O operations such as reading from files or network sockets. They provide a way to efficiently handle binary data in JavaScript, which is particularly useful in scenarios where performance and efficiency are important.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***what are express.json() and express.urlencoded() in express.js***

In Express.js, `express.json()` and `express.urlencoded()` are middleware functions that handle parsing incoming request bodies. They are commonly used for processing data submitted through HTML forms or sent as JSON in POST requests. Here's a brief explanation of each:

1. **`express.json()`**: This middleware is responsible for parsing incoming requests with JSON payloads. It parses the JSON data of the request body and makes it available in `req.body` property of the request object. It's typically used to handle AJAX requests or requests where the client sends JSON data in the body.

   Example usage:
   ```javascript
   const express = require('express');
   const app = express();

   app.use(express.json());

   app.post('/api/users', (req, res) => {
       console.log(req.body); // This will log the JSON data sent in the request body
       res.send('Data received successfully');
   });

   app.listen(3000, () => {
       console.log('Server is running on port 3000');
   });
   ```

2. **`express.urlencoded()`**: This middleware is responsible for parsing incoming requests with URL-encoded payloads. It parses the URL-encoded data of the request body and makes it available in `req.body` property of the request object. It's commonly used to handle form submissions from HTML forms.

   Example usage:
   ```javascript
   const express = require('express');
   const app = express();

   app.use(express.urlencoded({ extended: true }));

   app.post('/api/users', (req, res) => {
       console.log(req.body); // This will log the URL-encoded data sent in the request body
       res.send('Data received successfully');
   });

   app.listen(3000, () => {
       console.log('Server is running on port 3000');
   });
   ```

In both examples, the middleware functions `express.json()` and `express.urlencoded()` are applied to the Express application using `app.use()` to parse the incoming request bodies. This enables the application to access the parsed data conveniently in the `req.body` object for further processing. Depending on the type of data expected in the requests (JSON or URL-encoded), you can choose to use either or both of these middleware functions in your Express application.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***explain what is wrong withasync/await use in the forEach loop?***

Using `async/await` inside a `for` loop can lead to unexpected behavior if not used correctly. The primary issue arises from the fact that `await` pauses the execution of the current function until the awaited promise resolves. When you use `await` inside a loop, it will pause the loop until the awaited promise is resolved, which might not be the desired behavior.

Here's an example to illustrate the issue:

```javascript
async function example() {
  const array = [1, 2, 3];

  for (const item of array) {
    await someAsyncFunction(item);
    console.log(item);
  }
}

function someAsyncFunction(item) {
  return new Promise(resolve => {
    setTimeout(() => {
      console.log(`Processing ${item}`);
      resolve();
    }, 1000);
  });
}

example();
```

In this example, `someAsyncFunction` is an asynchronous function that simulates some asynchronous operation. The `example` function iterates over an array using a `for...of` loop and awaits the asynchronous operation for each item in the array.

The problem with this code is that it introduces unnecessary delays. The loop waits for each asynchronous operation to complete before moving to the next iteration. This behavior might not be what you want, especially if the operations are independent and could be executed concurrently.

To fix this issue, you can use `Promise.all` to execute all asynchronous operations concurrently:

```javascript
async function example() {
  const array = [1, 2, 3];

  await Promise.all(array.map(async item => {
    await someAsyncFunction(item);
    console.log(item);
  }));
}

function someAsyncFunction(item) {
  return new Promise(resolve => {
    setTimeout(() => {
      console.log(`Processing ${item}`);
      resolve();
    }, 1000);
  });
}

example();
```

In this modified version, `Promise.all` is used to await all asynchronous operations concurrently. This way, the operations will run in parallel, and the loop won't be blocked by waiting for each individual operation to complete.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***how would you read file parallel in node js provide code example***
To read multiple files in parallel in Node.js, you can utilize `Promise.all()` along with asynchronous file reading operations. Below is an example code snippet demonstrating how to achieve this:

```javascript
const fs = require('fs').promises;

// List of file paths to read
const filePaths = ['file1.txt', 'file2.txt', 'file3.txt'];

// Function to read a single file asynchronously
async function readFile(filePath) {
    try {
        const data = await fs.readFile(filePath, 'utf8');
        console.log(`Contents of ${filePath}:`, data);
        return data;
    } catch (error) {
        console.error(`Error reading ${filePath}:`, error);
        throw error;
    }
}

// Function to read all files in parallel
async function readFilesParallel(filePaths) {
    try {
        // Map file reading tasks to promises
        const fileReadingPromises = filePaths.map(filePath => readFile(filePath));

        // Wait for all file reading promises to resolve
        const fileContents = await Promise.all(fileReadingPromises);

        console.log('All files read successfully:', fileContents);
        return fileContents;
    } catch (error) {
        console.error('Error reading files in parallel:', error);
        throw error;
    }
}

// Call the function to read files in parallel
readFilesParallel(filePaths)
    .then(() => {
        console.log('All files have been read successfully!');
    })
    .catch(error => {
        console.error('Error:', error);
    });
```

In this example:

- We define a function `readFile()` that asynchronously reads the contents of a single file using `fs.readFile()` from the `fs.promises` API, which returns a promise.
- We define another function `readFilesParallel()` that takes an array of file paths as input, maps each file path to a call to `readFile()`, and then waits for all these promises to resolve using `Promise.all()`.
- Inside `readFilesParallel()`, we call `Promise.all()` with an array of promises returned by `readFile()`. This ensures that all file reading operations are executed concurrently.
- Finally, we call `readFilesParallel()` with an array of file paths and handle the resolved values in the `then()` block, or catch any errors in the `catch()` block.

This approach allows you to read multiple files in parallel efficiently, making use of Node.js's asynchronous capabilities.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***how would you read file sequence in node js provide code example***

To read files sequentially in Node.js, you can read each file one by one, waiting for each file to be read before moving on to the next one. You can achieve this by chaining promises or using async/await. Below is an example using async/await:

```javascript
const fs = require('fs').promises;

// List of file paths to read
const filePaths = ['file1.txt', 'file2.txt', 'file3.txt'];

// Function to read a single file asynchronously
async function readFile(filePath) {
    try {
        const data = await fs.readFile(filePath, 'utf8');
        console.log(`Contents of ${filePath}:`, data);
        return data;
    } catch (error) {
        console.error(`Error reading ${filePath}:`, error);
        throw error;
    }
}

// Function to read files sequentially using async/await
async function readFilesSequentially(filePaths) {
    try {
        const fileContents = [];
        // Loop through each file path and read it sequentially
        for (const filePath of filePaths) {
            const data = await readFile(filePath);
            fileContents.push(data);
        }
        console.log('All files read successfully:', fileContents);
        return fileContents;
    } catch (error) {
        console.error('Error reading files sequentially:', error);
        throw error;
    }
}

// Call the function to read files sequentially
(async () => {
    try {
        const result = await readFilesSequentially(filePaths);
        console.log('All files have been read successfully!', result);
    } catch (error) {
        console.error('Error:', error);
    }
})();
```

In this code:

- We define a function `readFilesSequentially()` that takes an array of file paths as input and reads each file sequentially using a `for...of` loop and `await` inside the loop.
- Inside the loop, `await` is used to wait for each file to be read before moving on to the next one.
- The `readFile()` function remains the same as in the previous example.
- We encapsulate the call to `readFilesSequentially()` within an immediately-invoked async function expression to allow the use of `await` at the top level of the script.


<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***when is it best to use module.exports vs a class to an object literal when defining nodejs module***
The choice between using `module.exports` and a class or object literal to define a Node.js module depends on several factors including the complexity of the module, the level of abstraction needed, and personal preference. Here's a breakdown of when it's best to use each approach:

1. **`module.exports`**:
   - Use `module.exports` when you want to export a single function or a simple value directly.
   - It's straightforward and suitable for exporting simple functionality, constants, or single functions.

   Example:
   ```javascript
   // calculator.js
   function add(a, b) {
       return a + b;
   }
   
   module.exports = add;
   ```

2. **Class**:
   - Use a class when you need to export a constructor function or a set of related functions and properties that logically belong together.
   - Classes are beneficial for encapsulating data and behavior into a cohesive unit, especially when you need to create multiple instances of the module.

   Example:
   ```javascript
   // user.js
   class User {
       constructor(name) {
           this.name = name;
       }
   
       greet() {
           console.log(`Hello, ${this.name}!`);
       }
   }
   
   module.exports = User;
   ```

3. **Object Literal**:
   - Use an object literal when you want to export multiple functions or properties that are related but don't necessarily need to be encapsulated within a class.
   - Object literals are useful when you want to group related functionality together without the need for instantiation.

   Example:
   ```javascript
   // utils.js
   const utils = {
       formatName(firstName, lastName) {
           return `${firstName} ${lastName}`;
       },
   
       generateRandomNumber(min, max) {
           return Math.floor(Math.random() * (max - min + 1) + min);
       }
   };
   
   module.exports = utils;
   ```

In summary, choose `module.exports` for simple exports, a class for encapsulating data and behavior into instances, and an object literal for grouping related functionality without the need for instantiation. The choice ultimately depends on the specific requirements and design considerations of your module.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>

## Q. ***How do I convert an existing callback API to promises?***

Converting an existing callback-based API to promises involves encapsulating the asynchronous operations within functions that return promises, allowing for cleaner and more readable asynchronous code using `async/await` or `.then()` syntax. Here's a general approach to convert a callback-based API to promises:

1. Identify the asynchronous functions in your API that currently accept callbacks.

2. Wrap each asynchronous function with a new function that returns a promise. This new function should accept the same arguments as the original function but not accept a callback parameter.

3. Inside the new function, perform the asynchronous operation using the original function, and resolve or reject the promise based on the success or failure of the operation.

4. Replace the usage of the callback-based functions with the new promise-based functions in your codebase.

Here's an example to illustrate this process:

Suppose you have an existing callback-based API with a function `doSomethingAsync`:

```javascript
function doSomethingAsync(callback) {
    // Simulate an asynchronous operation
    setTimeout(() => {
        const result = Math.random() < 0.5 ? 'success' : 'error';
        if (result === 'success') {
            callback(null, 'Data retrieved successfully');
        } else {
            callback(new Error('Failed to retrieve data'));
        }
    }, 1000);
}
```

To convert `doSomethingAsync` to a promise-based API, you can create a new function `doSomethingAsyncPromise`:

```javascript
function doSomethingAsyncPromise() {
    return new Promise((resolve, reject) => {
        doSomethingAsync((err, data) => {
            if (err) {
                reject(err);
            } else {
                resolve(data);
            }
        });
    });
}
```

Now, you can use `doSomethingAsyncPromise` with `async/await` or `.then()`:

```javascript
// Using async/await
async function fetchData() {
    try {
        const data = await doSomethingAsyncPromise();
        console.log(data);
    } catch (error) {
        console.error(error);
    }
}

fetchData();

// Using .then()
doSomethingAsyncPromise()
    .then(data => {
        console.log(data);
    })
    .catch(error => {
        console.error(error);
    });
```

This approach allows you to convert callback-based APIs to promise-based APIs, providing better readability and control flow in your asynchronous code.

<div align="right">
    <b><a href="#">↥ back to top</a></b>
</div>


